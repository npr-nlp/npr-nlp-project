{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe1ec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the things\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wrangle\n",
    "import acquire\n",
    "from prepare import basic_clean, tokenize, lemmatize, stem, remove_stopwords, prep_string_data#, split_data\n",
    "\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07320e93",
   "metadata": {},
   "source": [
    "# SPLITTING UP THE DATA HERE BASED ON A TSA APPROACH\n",
    "### Other of our explorations might work slightly differently, if we do a prediction model or topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08f5726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('npr_corpus.csv')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45972e72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = wrangle.get_npr_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be99cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.sort_values(by=['episode_id','episode_order'])[0:50]\n",
    "df.sort_values(by=['story_id_num','utterance_order'])[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78e2c5d",
   "metadata": {},
   "source": [
    "## Ok, since we already have utterance order, we can just use that as a feature\n",
    "## Moving on to utterance length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc6b3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.utterance[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1149a9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.utterance[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb47e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.utterance[0].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5161bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.utterance.apply(str.split).apply(len)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3fa098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['utterance_length'] = len(df.utterance.split())\n",
    "df['utterance_word_count'] = df.utterance.apply(str.split).apply(len)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd95f8a",
   "metadata": {},
   "source": [
    "#### A quick lesson on .apply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4abc997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_wordcloud(host):\n",
    "#     etc etc\n",
    "\n",
    "# h.apply(make_wordcloud)\n",
    "\n",
    "# the h is what is referred to after the apply is applied\n",
    "\n",
    "# or\n",
    "\n",
    "# def make_wordcloud(host, ajowpj):\n",
    "#     etc etc\n",
    "\n",
    "# h.apply(make_wordcloud, extr_words= blah blah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787cce9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_cleaning(s):\n",
    "    '''\n",
    "    Function to remove punctuation from word frequencies for hosts.\n",
    "    '''\n",
    "    # remove special characters\n",
    "    s = re.sub(r\"[^a-z0-9'\\s\\?]\", '', s)\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf2b7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.utterance[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441bb91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Series.str.contains(pat, case=True, flags=0, na=None, regex=True)\n",
    "\n",
    "df.utterance.str.contains(r\"[\\?]\", regex=True)[2]\n",
    "\n",
    "\n",
    "# Ok, so this indicates thee presence of a question mark...how about a question mark count?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0561a272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas.Series.str.count\n",
    "\n",
    "df.utterance.str.count(r\"[\\?]\")[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c56cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['question_mark_count'] = df.utterance.str.count(r\"[\\?]\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97082ee3",
   "metadata": {},
   "source": [
    "## Who asks the most questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c65ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835327d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c1fefd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "868c9016",
   "metadata": {},
   "source": [
    "## Is there a difference in the mean sentiment by speaker? Program? etc\n",
    "- Applied statistics-> i.e. stats testing. Is there a difference in the mean sentiment by speaker? Program? etc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01ead45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e2e69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6c4d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499385cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.date = pd.to_datetime(df.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7418211",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095f271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df['Date'].dt.strftime('%b-%Y')\n",
    "\n",
    "# df['episode_date'].dt.strftime('%b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e7c890",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['month']=df['date'].dt.strftime('%b')\n",
    "df['year']=pd.DatetimeIndex(df['date']).year\n",
    "df=df.reset_index()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087c164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c1c4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.set_index('date').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cdc0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.index.year >= 2005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09aeff46",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0d56cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[:'2015']\n",
    "validate = df['2016':'2017']\n",
    "test = df['2018':]\n",
    "train.shape, validate.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f56a024",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.index[-1:], validate.index[:1], validate.index[-1:], test.index[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a5ac70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.vader.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c3135f",
   "metadata": {},
   "source": [
    "## Using train split from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3d7d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_df = train[train.is_host==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc32516",
   "metadata": {},
   "outputs": [],
   "source": [
    "guest_df = train[train.is_host!=True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dd9b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_df.shape, guest_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523998e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffd4b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6fdd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# are there repeats in the host_df? \n",
    "host_df.speaker.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175b431e",
   "metadata": {},
   "source": [
    "- I can see two different duplicates for steve inskeep bc of typos\n",
    "- I think this is going to fall into the arena of significantly diminishing returns, and I will not address it at this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b191c970",
   "metadata": {},
   "outputs": [],
   "source": [
    "hosts_with_the_most = host_df.speaker.value_counts().head(10).index.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ccd683",
   "metadata": {},
   "source": [
    "- This is a list of the 12 hosts with the most observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a014d740",
   "metadata": {},
   "outputs": [],
   "source": [
    "hosts_with_the_most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eba46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_hosts_df = train[train.speaker.isin(hosts_with_the_most)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b083c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_hosts_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce48924",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_hosts_df.index.min(), top_hosts_df.index.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc442f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_hosts_df.vader.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117a5226",
   "metadata": {},
   "source": [
    "- Here, we have the average sentiment score for all the top hosts; as you can see, it is relatively neutral in sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b198ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.boxplot(data=top_hosts_df, x='speaker',y='vader')\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e09041e",
   "metadata": {},
   "source": [
    "- The mean sentiment value is awfully close for everyone here, so i'm going to stats test it with an ANOVA test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a953622",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_hypothesis = \"Average sentiment score is the same across hosts\"\n",
    "alternative_hypothesis = \"Average sentiment score is different in at least one host of the group\"\n",
    "alpha = 0.01 # Let's be 99% certain the result we see isn't due to chance/randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9dc284",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_df_list=[]\n",
    "for host in hosts_with_the_most:\n",
    "    x = host.split()\n",
    "    host_df_list.append(f'{x[0]}_df')\n",
    "print(host_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185ac6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hosts_with_the_most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a3e1bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's isolate our hosts\n",
    "NEAL_df = train[train.speaker == 'neal conan, host'].vader\n",
    "IRA_df = train[train.speaker ==  'ira flatow, host'].vader\n",
    "ROBERT_df = train[train.speaker == 'robert siegel, host'].vader\n",
    "STEVE_df = train[train.speaker == 'steve inskeep, host'].vader\n",
    "MELISSA_df = train[train.speaker == 'melissa block, host'].vader\n",
    "FARAI_df = train[train.speaker ==  'farai chideya, host'].vader\n",
    "RENEE_df = train[train.speaker == 'renee montagne, host'].vader\n",
    "SCOTT_df = train[train.speaker == 'scott simon, host'].vader\n",
    "DAVID_df = train[train.speaker == 'david greene, host'].vader\n",
    "RACHEL_df = train[train.speaker == 'rachel martin, host'].vader\n",
    "\n",
    "\n",
    "# GUY_df = train[train.speaker == 'guy raz, host'].vader_stopped\n",
    "# MADELEINE_df = train[train.speaker == 'madeleine brand, host'].vader_stopped\n",
    "# MICHELE_df = train[train.speaker == 'michelle norris, host'].vader_stopped\n",
    "# ALEX_df = train[train.speaker == 'alex chadwick, host'].vader_stopped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93229f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_oneway is our ANOVA test\n",
    "# See https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.f_oneway.html for more info\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "f, p = f_oneway(NEAL_df, IRA_df, ROBERT_df, STEVE_df, MELISSA_df, FARAI_df, RENEE_df, SCOTT_df, DAVID_df, RACHEL_df)\n",
    "f, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce47908e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if p < alpha:\n",
    "    print(\"We reject the null hypothesis that\", null_hypothesis)\n",
    "    print(\"We move forward with the alternative hypothesis that\", alternative_hypothesis)\n",
    "else:\n",
    "    print(\"We fail to reject the null hypothesis\")\n",
    "    print(\"Evidence does not support the claim that sentiment differs from host to host\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59be5657",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"speaker\", y=\"vader\", kind=\"bar\", data=top_hosts_df)\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41185a68",
   "metadata": {},
   "source": [
    "- Another view of the same...it looks more clear here that there is a difference, however note the small scale on the y axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dbdfdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b693fd81",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "top_hosts_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb07f53",
   "metadata": {},
   "source": [
    "### The conclusion to the above is that there is a statistically significant difference in sentiment in the top hosts group, even if it is small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c767bed",
   "metadata": {},
   "source": [
    "### Josh's code for the hosts' words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877405c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# host_words = train[train.speaker.isin(hosts_with_the_most)].groupby('speaker')['lemmatized'].agg(lambda col: ' '.join(col))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe7e1bd",
   "metadata": {},
   "source": [
    "## What's the difference in sentiment score between hosts and non-hosts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fc1764",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_host_df = train[train.is_host==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fccf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure\n",
    "sns.boxplot(data=train, x='is_host',y='vader')\n",
    "plt.title(\"The mean sentiment score for Hosts versus Non-Hosts\")\n",
    "plt.ylabel(\"Mean Sentiment Score\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b528982",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train.is_host==False].vader.mean(), train[train.is_host==True].vader.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cdd3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_hypothesis = \"Hosts and Non-hosts have the same sentiment score\"\n",
    "alternative_hypothesis = \"Hosts and Non-hosts have different sentiment scores\"\n",
    "alpha = 0.01 # we want to be 99% sure our results aren't the result of chance/randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96853419",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stats testing on the same\n",
    "# anova is the wrong test!\n",
    "from scipy import stats\n",
    "\n",
    "t, p = stats.ttest_ind(host_df.vader,non_host_df.vader)\n",
    "t, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e460de7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if p < alpha:\n",
    "    print(\"We reject the null hypothesis that\", null_hypothesis)\n",
    "    print(\"We move forward with the hypothesis that\", alternative_hypothesis)\n",
    "else:\n",
    "    print(\"We fail to reject the null hypothesis\")\n",
    "    print(f\"Evidence does not support the claim that\", alternate_hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52c9128",
   "metadata": {},
   "source": [
    "### The average sentiment score for non-hosts is somewhat higher than for the hosts.  To be expected for a relatively neutral news outlet?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d51fdca",
   "metadata": {},
   "source": [
    "## How about sentiment score by year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2423e432",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.resample('Y').vader.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0f91bf",
   "metadata": {},
   "source": [
    "- why don't we have vader scores in 2000-2003 on yearly resampling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e12c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_yearly = pd.DataFrame(train.resample('Y').vader.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df03bc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_yearly.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd3f24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_monthly = pd.DataFrame(train.resample('M').vader.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db450cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_monthly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac045d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_monthly.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544fb872",
   "metadata": {},
   "source": [
    "- What's with all those missing sentiment scores in the early aughts?\n",
    "- We eliminated observations pre-2005, they were missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87588e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(df.resample(\"d\").vader_stopped.mean()).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da850e8",
   "metadata": {},
   "source": [
    "## By day of week?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8708ca1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train.groupby(train.index.weekday).vader.mean()\n",
    "# sentiment by day, 0 is monday, 6 is sunday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175e5cdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.boxplot(data=train, x=train.index.weekday,y='vader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25e8e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Monday_df = train[train.index.weekday == 0]\n",
    "Tuesday_df = train[train.index.weekday == 1]\n",
    "Wednesday_df = train[train.index.weekday == 2]\n",
    "Thursday_df = train[train.index.weekday == 3]\n",
    "Friday_df = train[train.index.weekday == 4]\n",
    "Saturday_df = train[train.index.weekday == 5]\n",
    "Sunday_df = train[train.index.weekday == 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eddd740",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_hypothesis = \"Different days of the week have the same sentiment score\"\n",
    "alternative_hypothesis = \"At least one day of the week has a different sentiment score from the rest\"\n",
    "alpha = 0.01 # we want to be 99% sure our results aren't the result of chance/randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87466b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "f, p = f_oneway(Monday_df.vader,\\\n",
    "Tuesday_df.vader,\\\n",
    "Wednesday_df.vader,\\\n",
    "Thursday_df.vader,\\\n",
    "Friday_df.vader,\\\n",
    "Saturday_df.vader,\\\n",
    "Sunday_df.vader)\n",
    "\n",
    "f, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b40570",
   "metadata": {},
   "outputs": [],
   "source": [
    "if p < alpha:\n",
    "    print(\"We reject the null hypothesis that\", null_hypothesis)\n",
    "    print(\"We move forward with the alternative hypothesis that\", alternative_hypothesis)\n",
    "else:\n",
    "    print(\"We fail to reject the null hypothesis\")\n",
    "    print(\"Evidence does not support the claim that sentiment differs from one weekday to the next\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fdcd7b",
   "metadata": {},
   "source": [
    "## And by program?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c308fbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.program.value_counts().index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9406a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Talk_of_the_Nation_df = train[train.program == 'talk of the nation']\n",
    "Morning_Edition_df = train[train.program == 'morning edition']\n",
    "All_Things_Considered_df = train[train.program == 'all things considered']\n",
    "News_and_Notes_df = train[train.program == 'news & notes']\n",
    "Weekend_Edition_Saturday_df = train[train.program == 'weekend edition saturday']\n",
    "Weekend_Edition_Sunday_df = train[train.program == 'weekend edition sunday']\n",
    "Day_to_Day_df = train[train.program == 'day to day']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebda05fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,10))\n",
    "Talk_of_the_Nation_df.resample(\"y\").vader.mean().plot(alpha=.5)\n",
    "All_Things_Considered_df.resample(\"y\").vader.mean().plot(alpha=.5)\n",
    "Morning_Edition_df.resample(\"y\").vader.mean().plot(alpha=.5)\n",
    "News_and_Notes_df.resample(\"y\").vader.mean().plot(alpha=.5)\n",
    "Day_to_Day_df.resample(\"y\").vader.mean().plot(alpha=.5)\n",
    "Weekend_Edition_Sunday_df.resample(\"y\").vader.mean().plot(alpha=.5)\n",
    "Weekend_Edition_Saturday_df.resample(\"y\").vader.mean().plot(alpha=.5)\n",
    "plt.title(\"Average Sentiment over Time, by Program\")\n",
    "plt.legend(['Talk of the Nation',\n",
    " 'All Things Considered',\n",
    " 'Morning Edition',\n",
    " 'News & Notes',\n",
    " 'Day to Day',\n",
    " 'Weekend Edition Sunday',\n",
    " 'Weekend Edition Saturday'], prop={'size': 20})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd8348a",
   "metadata": {},
   "source": [
    "- Note the programs that have gone off air\n",
    "- Also, there is some difference in sentiment from program to program\n",
    "- Weekend Edition (Sat and Sun) have higher sentiment scores--these programs try to be a little on the lighter side for the weekend\n",
    "- We're definitely starting off the day on the most pessimistic note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaee2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_hypothesis = \"The different programs have the same sentiment score\"\n",
    "alternative_hypothesis = \"At least one program has a different sentiment score\"\n",
    "alpha = 0.01 # we want to be 99% sure our results aren't the result of chance/randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b14e0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_oneway is our ANOVA test\n",
    "# See https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.f_oneway.html for more info\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "f, p = f_oneway(Talk_of_the_Nation_df.vader,\n",
    "All_Things_Considered_df.vader,\n",
    "Morning_Edition_df.vader,\n",
    "News_and_Notes_df.vader,\n",
    "Day_to_Day_df.vader,\n",
    "Weekend_Edition_Sunday_df.vader,\n",
    "Weekend_Edition_Saturday_df.vader,)\n",
    "f, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c0959f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if p < alpha:\n",
    "    print(\"We reject the null hypothesis that\", null_hypothesis)\n",
    "    print(\"We move forward with the alternative hypothesis that\", alternative_hypothesis)\n",
    "else:\n",
    "    print(\"We fail to reject the null hypothesis\")\n",
    "    print(\"Evidence does not support the claim that sentiment differs from host to host\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f5b45c",
   "metadata": {},
   "source": [
    "## How did sentiment change after 9/11 or etc...\n",
    "- after obama election? \n",
    "- from start to finish of 2016 presidential campaign?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0af9d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569e03a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57d97d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4b7e96a",
   "metadata": {},
   "source": [
    "## what about clustering?\n",
    "- There is very little continuous data here, clusering might not be possible.\n",
    "- datetime...but how usefull can that be? \n",
    "- utterance order? is that available?\n",
    "- maybe explor utterance order vs utterance length vs number of question marks or similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3951aae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8540c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.sort_values(by=['story_id_num','utterance_order'])[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424fe631",
   "metadata": {},
   "source": [
    "### Ok, for the first time I'm seeing clearly how the episodes are in fact ordered by 'story' (episode_id) and then ordered by utterance within the story\n",
    "\n",
    "### This is great, bc it's what is going to make topic modeling possible down the line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac73904",
   "metadata": {},
   "source": [
    "# First...\n",
    "- Let's do a tsa model of sentiment using Prophet : )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34edfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6076b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for_tsa_modeling_df = pd.DataFrame(df['vader'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ffefc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for_tsa_modeling_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92b5231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for_prophet_df.resample('Y').mean()\n",
    "for_tsa_modeling_df.index.year.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbe4542",
   "metadata": {},
   "source": [
    "- there is NOTHING in there fro 2000 to 2003\n",
    "- 2004 barely a thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bb9de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tsa_train = for_tsa_modeling_df['2005':'2015']\n",
    "tsa_validate = for_tsa_modeling_df['2016':'2017']\n",
    "tsa_test = for_tsa_modeling_df['2018':]\n",
    "tsa_train.shape, tsa_validate.shape, tsa_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f285f9b7",
   "metadata": {},
   "source": [
    "- I've dropped all observations prior to 2005 in this split due to the problem with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaea6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsa_train[-1:], tsa_validate[:1], tsa_validate[-1:], tsa_test[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d80e8d",
   "metadata": {},
   "source": [
    "- the splits look good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069ff495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in prophet_train.columns:\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(tsa_train.resample('M').vader.mean())\n",
    "plt.plot(tsa_validate.resample('M').vader.mean())\n",
    "plt.plot(tsa_test.resample('M').vader.mean())\n",
    "plt.ylabel('Sentiment Score')\n",
    "plt.title('Time Series Plot of Sentiment, by Train/Validate/Test')\n",
    "plt.legend(labels = [\"Train\",\"Validate\",\"Test\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc522546",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.autocorrelation_plot(tsa_train.resample('y').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217652e3",
   "metadata": {},
   "source": [
    "- There doesn't look to be any seasonality to speak of--try running the autocorrelation_plot with different resmample lengths\n",
    "- from the curriculum:\n",
    "    - \"The dashed lines are a way measure whether the observed autocorrelation is a meaningful signal or just white noise. If the majority of the peaks and valleys fall within the dashed lines, the time series is probably white noise.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933649a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.api import Holt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b41d17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prophet_train.dropna(axis=0, how='any')\n",
    "tsa_train.dropna(axis=0, how='any').resample('M').mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df11b2d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tsa_train[tsa_train.vader.isnull() == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662077bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsa_train['vader_stopped'] = tsa_train['vader_stopped'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd5b000",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sm.tsa.seasonal_decompose(tsa_train.resample('M').mean()).plot()\n",
    "\n",
    "\n",
    "# _ = sm.tsa.seasonal_decompose(tsa_train['vader_stopped'].resample('W').mean()).plot()\n",
    "\n",
    "\n",
    "# ValueError: This function does not handle missing values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4452b9",
   "metadata": {},
   "source": [
    "### Simple Average Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773f3c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fc50f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the empty dataframe\n",
    "eval_df = pd.DataFrame(columns=['model_type', 'target_var', 'rmse'])\n",
    "\n",
    "# function to store rmse for comparison purposes\n",
    "def append_eval_df(model_type, target_var):\n",
    "    rmse = evaluate(target_var)\n",
    "    d = {'model_type': [model_type], 'target_var': [target_var], 'rmse': [rmse]}\n",
    "    d = pd.DataFrame(d)\n",
    "    return eval_df.append(d, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c3113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation function to compute rmse\n",
    "def evaluate(target_var):\n",
    "    rmse = round(sqrt(mean_squared_error(validate[target_var], yhat_df[target_var])), 4)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8179505f",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = round(tsa_train['vader'].mean(), 4)\n",
    "\n",
    "def make_predictions():\n",
    "    yhat_df = pd.DataFrame({'vader': score}, index = validate.index)\n",
    "\n",
    "    return yhat_df\n",
    "\n",
    "yhat_df = make_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229c3bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot and evaluate \n",
    "def plot_and_eval(target_var):\n",
    "    plt.figure(figsize = (12,4))\n",
    "    plt.plot(tsa_train[target_var].resample('M').mean(), label = 'Train', linewidth = 1)\n",
    "    plt.plot(tsa_validate[target_var].resample('M').mean(), label = 'Validate', linewidth = 1)\n",
    "    plt.plot(yhat_df[target_var])\n",
    "    plt.title(target_var)\n",
    "    rmse = evaluate(target_var)\n",
    "    print(target_var, '-- RMSE: {:.4f}'.format(rmse))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b025641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot and evaluate \n",
    "def plot_and_eval_2(target_var,model_type):\n",
    "    plt.figure(figsize = (12,4))\n",
    "    plt.plot(tsa_train[target_var].resample('M').mean(), label = 'Train', linewidth = 1)\n",
    "    plt.plot(tsa_validate[target_var].resample('M').mean(), label = 'Validate', linewidth = 1)\n",
    "    plt.plot(yhat_df[target_var])\n",
    "    plt.title(f\"{target_var}, {model_type}\")\n",
    "    rmse = evaluate(target_var)\n",
    "    print(target_var, '-- RMSE: {:.4f}'.format(rmse))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de2d3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7957e757",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_and_eval('vader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994c5295",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = append_eval_df(model_type='simple_average', \n",
    "                             target_var = 'vader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ebfe40",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353bb4ad",
   "metadata": {},
   "source": [
    "### Moving Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d666c1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['vader'].rolling(3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3c7db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying with different periods\n",
    "\n",
    "periods = [30, 60, 90, 365]\n",
    "\n",
    "for p in periods:\n",
    "    score = round(tsa_train['vader'].rolling(p).mean().iloc[-1], 4)\n",
    "    yhat_df = make_predictions()\n",
    "    model_type = str(p) + ' day moving average'\n",
    "    eval_df = append_eval_df(model_type = model_type,\n",
    "                             target_var = 'vader'\n",
    "                            )\n",
    "    plot_and_eval_2('vader', model_type)\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dadcfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62538afd",
   "metadata": {},
   "source": [
    "# In case you missed it: there's something wrong with the moving average models.  Or am I wrong?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84727c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = sm.tsa.seasonal_decompose(train[col].resample('W').mean()).plot()\n",
    "# plt.show()\n",
    "\n",
    "# Still having problems loooking at seasonl decomposition due to missing valuese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa606a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsa_validate.index[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff791bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsa_train['vader'].resample('M').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd85d9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Holt(tsa_train['vader'], exponential = False)\n",
    "# model = model.fit(smoothing_level = .1, \n",
    "#                   smoothing_slope = .1, \n",
    "#                   optimized = False)\n",
    "# yhat_items = model.predict(start = tsa_validate.index[0], \n",
    "#                            end = tsa_validate.index[-1])\n",
    "# yhat_df['vader'] = round(yhat_items, 4)\n",
    "\n",
    "# #HOLTS IS BROKEN : (\n",
    "\n",
    "# # KeyError: 'The `start` argument could not be matched to a location related to the index of the data.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124e9d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yhat_df.head()\n",
    "# the above is returning NaNs only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cc40fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_and_eval('vader')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c33438",
   "metadata": {},
   "source": [
    "### Moving prophet model to its own notebook bc of the size of it (not pushing to github)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36e1406",
   "metadata": {},
   "source": [
    "## The confidence intervals on the Prophet model were massive.  The sentiment observations in this data are very close together and yield rather little predictive power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cb2846",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsa_train.groupby([tsa_train.index.year, tsa_train.index.month]).mean().unstack(0).plot()\n",
    "plt.legend(loc=(1.3,.5), bbox_to_anchor=(0.5, 0., 0.5, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3c6f66",
   "metadata": {},
   "source": [
    "- Is there a trend of dipping sentiment in the summer?\n",
    "- Also, it seems like the vader scores are lower in the later years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199b7b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37460be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d29ffb2",
   "metadata": {},
   "source": [
    "# MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805f503a",
   "metadata": {},
   "source": [
    "### BASELINE:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95a5f91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b47a3f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5f0ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS WAS HANDY CODE TO ELIMINATE SPEAKERS WITH ONLY 1 OR 2 OBSERVATIONS\n",
    "# THOSE SPEAKERS WERE CREATING PROBLEMS WITH SPLITTING\n",
    "# df.groupby('speaker').filter(lambda x : len(x)<=2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36fbf62",
   "metadata": {},
   "source": [
    "## In the next cell, i've sampled down the original df and derived all the splits based on 'rest', which was based on the small_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a78ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df = df.sample(10_000)\n",
    "\n",
    "counts = small_df['speaker'].value_counts()\n",
    "\n",
    "rest = small_df[~small_df['speaker'].isin(counts[counts < 3].index)]\n",
    "# df[df.groupby('speaker').filter(lambda x : len(x)<=2)] #this code doesn't work\n",
    "\n",
    "# res works perfectly : ))))))))))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e18ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62320e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape,rest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfd70e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rest.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d66d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rest = rest.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d30150c",
   "metadata": {},
   "source": [
    "# Trying a new approach to splitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed35685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(rest.lemmatized) # is this a problem? to fit on the whole df? \n",
    "#instead of just X? Been meaning to ask a while, this is from lesson\n",
    "y = rest.is_host\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=.2)\n",
    "\n",
    "train = pd.DataFrame(dict(actual=y_train))\n",
    "test = pd.DataFrame(dict(actual=y_test))\n",
    "\n",
    "lm = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "train['predicted'] = lm.predict(X_train)\n",
    "test['predicted'] = lm.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd1ce05",
   "metadata": {},
   "source": [
    "## Thank god, we have a model.  Built on TF-IDF and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a3a68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: {:.2%}'.format(accuracy_score(train.actual, train.predicted)))\n",
    "print('---')\n",
    "print('Confusion Matrix')\n",
    "print(pd.crosstab(train.predicted, train.actual))\n",
    "print('---')\n",
    "print(classification_report(train.actual, train.predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fe3e9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# classification_report(train.actual, train.predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b9ec4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X= cv.fit_transform(rest.lemmatized)\n",
    "y = rest.is_host\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=.2)\n",
    "\n",
    "train = pd.DataFrame(dict(actual=y_train))\n",
    "test = pd.DataFrame(dict(actual=y_test))\n",
    "\n",
    "lm = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "train['predicted'] = lm.predict(X_train)\n",
    "test['predicted'] = lm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4d8d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: {:.2%}'.format(accuracy_score(train.actual, train.predicted)))\n",
    "print('---')\n",
    "print('Confusion Matrix')\n",
    "print(pd.crosstab(train.predicted, train.actual))\n",
    "print('---')\n",
    "print(classification_report(train.actual, train.predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791b13fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1064c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367e7eba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec52d4aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215f4923",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6ab0f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281acada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6b6225",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbab23db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b60552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d28623d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83944a07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
