{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05d49a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the imports used throughout the notebook:\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wrangle\n",
    "import acquire\n",
    "import model\n",
    "from prepare import basic_clean, tokenize, lemmatize, stem, remove_stopwords, prep_string_data#, split_data\n",
    "import scipy as sp\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "import nltk\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8343b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The df has 1889857 rows and 12 columns.\n",
      "train shape is (42327, 13)\n",
      "validate shape is (18141, 13)\n",
      "test shape is (15118, 13)\n",
      "Basic Splits Created\n",
      "TF_IDF Splits Created\n",
      "Count Vectorizer Splits Created\n",
      "Logistic Regression Beginning\n",
      "LR done\n",
      "Decision Tree beginning\n",
      "DT done\n",
      "Random Forest beginning\n",
      "RF done\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model_Type</th>\n",
       "      <th>Train_Accuracy</th>\n",
       "      <th>Validate_Accuracy</th>\n",
       "      <th>Accuracy_Difference</th>\n",
       "      <th>Beats_Baseline_By</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForest_CountVectorizer_Plus_Features</td>\n",
       "      <td>0.660075</td>\n",
       "      <td>0.657681</td>\n",
       "      <td>0.002394</td>\n",
       "      <td>0.157067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Model_Type  Train_Accuracy  \\\n",
       "0  RandomForest_CountVectorizer_Plus_Features        0.660075   \n",
       "\n",
       "   Validate_Accuracy  Accuracy_Difference  Beats_Baseline_By  \n",
       "0           0.657681             0.002394           0.157067  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.run_modeling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ea9c146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an empty evaluation dataframe\n",
    "# eval_df = pd.DataFrame(columns=['Model_Type', 'Train_Accuracy','Validate_Accuracy','Accuracy_Difference', 'Beats_Baseline_By'])\n",
    "\n",
    "# # function to store accuracy for comparison purposes\n",
    "# def append_eval_df(model_type, train_accuracy, validate_accuracy):\n",
    "#     d = {'Model_Type': [model_type],'Train_Accuracy':[train_accuracy], \\\n",
    "#         'Validate_Accuracy': [validate_accuracy], \\\n",
    "#             'Accuracy_Difference': [train_accuracy - validate_accuracy], \\\n",
    "#                 'Beats_Baseline_By':[validate_accuracy - 0.500614]}  # let's try to do this programatically\n",
    "#     d = pd.DataFrame(d)\n",
    "#     return eval_df.append(d, ignore_index = True)\n",
    "\n",
    "\n",
    "\n",
    "# df = wrangle.get_npr_data()\n",
    "# df['question_mark_count'] = df.utterance.str.count(r\"[\\?]\")\n",
    "# df['utterance_word_count'] = df.utterance.apply(str.split).apply(len)\n",
    "\n",
    "# # sample down due to size issuees\n",
    "# small_df = df.sample(100_000, random_state=222)\n",
    "# # counts of observations per speaker\n",
    "# counts = small_df['speaker'].value_counts()\n",
    "# # limiting our df to only speakers with 3 or more utterances. This helped wheen stratifying the splits\n",
    "# rest = small_df[~small_df['speaker'].isin(counts[counts < 3].index)]\n",
    "\n",
    "# # get initial splits based no \"rest\", which is a sampled-down df with speakers with 2 or less observations removed\n",
    "# train, validate, test = wrangle.split_data(rest)\n",
    "# print(f\"train shape is {train.shape}\")\n",
    "# print(f\"validate shape is {validate.shape}\")\n",
    "# print(f\"test shape is {test.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "# # define the features to go into the model\n",
    "# features = train.drop(columns = ['story_id_num','speaker','utterance','program','title',\\\n",
    "#     'is_host','clean','lemmatized']).columns.to_list()\n",
    "\n",
    "# # X_ and y_ splits\n",
    "# X_train = train[features]\n",
    "# y_train = train.is_host\n",
    "\n",
    "# X_validate = validate[features]\n",
    "# y_validate = validate.is_host\n",
    "\n",
    "# X_test = test[features]\n",
    "# y_test = test.is_host\n",
    "# print(\"Basic Splits Created\")\n",
    "\n",
    "# # add a column to each basic split for the \"actual\" result (i.e. y_)\n",
    "# train['actual'] = y_train\n",
    "# validate['actual'] = y_validate\n",
    "# test['actual'] = y_test\n",
    "\n",
    "# # create splits including TF-IDF features\n",
    "# # assign the tfidf vectorizer\n",
    "# tfidf = TfidfVectorizer()\n",
    "# # fit/transform vectorizer on train only\n",
    "# X_train_tfidf = tfidf.fit_transform(train.lemmatized) \n",
    "# X_validate_tfidf = tfidf.transform(validate.lemmatized)\n",
    "# X_test_tfidf = tfidf.transform(test.lemmatized)\n",
    "# # add features to X_tfidf splits\n",
    "# X_train_tfidf_plusfeatures = sp.sparse.hstack((X_train_tfidf, pd.DataFrame(X_train.question_mark_count),\\\n",
    "#     pd.DataFrame(X_train.utterance_order),pd.DataFrame(X_train.vader),pd.DataFrame(X_train.utterance_word_count)))\n",
    "# X_validate_tfidf_plusfeatures = sp.sparse.hstack((X_validate_tfidf, pd.DataFrame(X_validate.question_mark_count),\\\n",
    "#     pd.DataFrame(X_validate.utterance_order),pd.DataFrame(X_validate.vader),pd.DataFrame(X_validate.utterance_word_count)))\n",
    "# X_test_tfidf_plusfeatures = sp.sparse.hstack((X_test_tfidf, pd.DataFrame(X_test.question_mark_count),\\\n",
    "#     pd.DataFrame(X_test.utterance_order),pd.DataFrame(X_test.vader),pd.DataFrame(X_test.utterance_word_count)))\n",
    "# print('TF_IDF Splits Created')\n",
    "# # Crate splits with count vectorized features\n",
    "# # define the model #cell 156  in  the  noteebook\n",
    "# cv = CountVectorizer()\n",
    "# # fit/transform vectorizer on train only\n",
    "# X_train_cv = cv.fit_transform(train.lemmatized) \n",
    "# X_validate_cv = cv.transform(validate.lemmatized)\n",
    "# X_test_cv = cv.transform(test.lemmatized)\n",
    "# # add features to X_cv splits\n",
    "# X_train_cv_plusfeatures = sp.sparse.hstack((X_train_cv, pd.DataFrame(X_train.question_mark_count),\\\n",
    "#     pd.DataFrame(X_train.utterance_order),pd.DataFrame(X_train.vader),pd.DataFrame(X_train.utterance_word_count)))\n",
    "# X_validate_cv_plusfeatures = sp.sparse.hstack((X_validate_cv, pd.DataFrame(X_validate.question_mark_count),\\\n",
    "#     pd.DataFrame(X_validate.utterance_order),pd.DataFrame(X_validate.vader),pd.DataFrame(X_validate.utterance_word_count)))\n",
    "# X_test_cv_plusfeatures = sp.sparse.hstack((X_test_cv, pd.DataFrame(X_test.question_mark_count),\\\n",
    "#     pd.DataFrame(X_test.utterance_order),pd.DataFrame(X_test.vader),pd.DataFrame(X_test.utterance_word_count)))\n",
    "# print('Count Vectorizer Splits Created')\n",
    "\n",
    "# # create baseline on mode (is_host == True)\n",
    "# train['baseline_pred'] = True\n",
    "# validate['baseline_pred'] = True\n",
    "# test['baseline_pred'] = True\n",
    "\n",
    "# # append to eval df\n",
    "# eval_df = append_eval_df('baseline_pred', accuracy_score(train.is_host, train.baseline_pred),accuracy_score(validate.is_host, validate.baseline_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a708e63c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
