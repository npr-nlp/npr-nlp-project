{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the things\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import acquire\n",
    "from prepare import basic_clean, tokenize, lemmatize, stem, remove_stopwords\n",
    "import prepare\n",
    "import wrangle \n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Planning Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>episode_order</th>\n",
       "      <th>speaker</th>\n",
       "      <th>utterance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57264</td>\n",
       "      <td>9</td>\n",
       "      <td>Ms. LOREN MOONEY (Editor-in-Chief, Bicycling M...</td>\n",
       "      <td>It's a 2,200-mile race. To give some sense of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57264</td>\n",
       "      <td>10</td>\n",
       "      <td>Ms. LOREN MOONEY (Editor-in-Chief, Bicycling M...</td>\n",
       "      <td>So for a top competitor like Lance to try to m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57264</td>\n",
       "      <td>11</td>\n",
       "      <td>NEAL CONAN, host</td>\n",
       "      <td>So in every team, presumably there's one star,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>57264</td>\n",
       "      <td>12</td>\n",
       "      <td>Ms. LOREN MOONEY (Editor-in-Chief, Bicycling M...</td>\n",
       "      <td>That's right. Each team has nine riders. And w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57264</td>\n",
       "      <td>13</td>\n",
       "      <td>NEAL CONAN, host</td>\n",
       "      <td>So slipstream, this is like drafting in car ra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode  episode_order                                            speaker  \\\n",
       "0    57264              9  Ms. LOREN MOONEY (Editor-in-Chief, Bicycling M...   \n",
       "1    57264             10  Ms. LOREN MOONEY (Editor-in-Chief, Bicycling M...   \n",
       "2    57264             11                                   NEAL CONAN, host   \n",
       "3    57264             12  Ms. LOREN MOONEY (Editor-in-Chief, Bicycling M...   \n",
       "4    57264             13                                   NEAL CONAN, host   \n",
       "\n",
       "                                           utterance  \n",
       "0  It's a 2,200-mile race. To give some sense of ...  \n",
       "1  So for a top competitor like Lance to try to m...  \n",
       "2  So in every team, presumably there's one star,...  \n",
       "3  That's right. Each team has nine riders. And w...  \n",
       "4  So slipstream, this is like drafting in car ra...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # read in primary utterances csv for dataframe\n",
    "# df = pd.read_csv('./utterances.csv')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19633    569\n",
       "35108    552\n",
       "57481    539\n",
       "73336    492\n",
       "66523    480\n",
       "        ... \n",
       "80400      1\n",
       "61102      1\n",
       "88047      1\n",
       "73497      1\n",
       "92554      1\n",
       "Name: episode, Length: 104920, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # find utterance count per episode\n",
    "# df.episode.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1357"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # find utterance count per episode\n",
    "# test = df.episode.value_counts() < 2\n",
    "# test.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pull in episodes df\n",
    "# ep_df = pd.read_csv('./episodes.csv')\n",
    "# ep_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Good morning. I'm David Greene. Here's the weather forecast for Albuquerque, New Mexico today - cloudy with a chance of grasshoppers. Yes, that's right. The animals are swarming the city so densely, local radar is mistaking them for rain crowds. These insects are hovering in masses as high as a thousand feet up. Officials say this is the worst infestation in 20 years, but it should pass soon. With any luck, it will go back to raining rain - helping to clean up the mess left behind by these grasshoppers. You're listening to MORNING EDITION.\"]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # check episode content to see if it matches episode id\n",
    "# df[df.episode == 92554].utterance.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Albuquerque's Weather? Cloudy With A Chance of Grasshoppers\"]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # check ep_df.id = df.episode\n",
    "# ep_df[ep_df.id == 92554].title.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial takeaways\n",
    "There are episodes with one utterance, showing less interview setting and more reporting.\n",
    "\n",
    "ep_df.id == df.episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('./utterances.csv')\n",
    "# ep_df = pd.read_csv('./episodes.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['speaker'] = df.speaker.str.lower()\n",
    "# df['program'] = df.program.str.lower()\n",
    "# df['title'] = df.title.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # joining utterances df ('df') and episodes on 'id'\n",
    "# joined_df = pd.merge(df, ep_df, left_on = 'episode', right_on='id', how = 'inner')\n",
    "# joined_df.drop(columns = ['id'], inplace=True)\n",
    "# joined_df.rename(columns={'episode':'episode_id'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joined_df['is_host'] = joined_df.speaker.str.contains(r'\\W*(host)\\W*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>episode_order</th>\n",
       "      <th>speaker</th>\n",
       "      <th>utterance</th>\n",
       "      <th>program</th>\n",
       "      <th>title</th>\n",
       "      <th>episode_date</th>\n",
       "      <th>is_host</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57264</td>\n",
       "      <td>9</td>\n",
       "      <td>ms. loren mooney (editor-in-chief, bicycling m...</td>\n",
       "      <td>It's a 2,200-mile race. To give some sense of ...</td>\n",
       "      <td>talk of the nation</td>\n",
       "      <td>how to watch the tour de france</td>\n",
       "      <td>2010-07-12</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57264</td>\n",
       "      <td>10</td>\n",
       "      <td>ms. loren mooney (editor-in-chief, bicycling m...</td>\n",
       "      <td>So for a top competitor like Lance to try to m...</td>\n",
       "      <td>talk of the nation</td>\n",
       "      <td>how to watch the tour de france</td>\n",
       "      <td>2010-07-12</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57264</td>\n",
       "      <td>11</td>\n",
       "      <td>neal conan, host</td>\n",
       "      <td>So in every team, presumably there's one star,...</td>\n",
       "      <td>talk of the nation</td>\n",
       "      <td>how to watch the tour de france</td>\n",
       "      <td>2010-07-12</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>57264</td>\n",
       "      <td>12</td>\n",
       "      <td>ms. loren mooney (editor-in-chief, bicycling m...</td>\n",
       "      <td>That's right. Each team has nine riders. And w...</td>\n",
       "      <td>talk of the nation</td>\n",
       "      <td>how to watch the tour de france</td>\n",
       "      <td>2010-07-12</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57264</td>\n",
       "      <td>13</td>\n",
       "      <td>neal conan, host</td>\n",
       "      <td>So slipstream, this is like drafting in car ra...</td>\n",
       "      <td>talk of the nation</td>\n",
       "      <td>how to watch the tour de france</td>\n",
       "      <td>2010-07-12</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode_id  episode_order  \\\n",
       "0       57264              9   \n",
       "1       57264             10   \n",
       "2       57264             11   \n",
       "3       57264             12   \n",
       "4       57264             13   \n",
       "\n",
       "                                             speaker  \\\n",
       "0  ms. loren mooney (editor-in-chief, bicycling m...   \n",
       "1  ms. loren mooney (editor-in-chief, bicycling m...   \n",
       "2                                   neal conan, host   \n",
       "3  ms. loren mooney (editor-in-chief, bicycling m...   \n",
       "4                                   neal conan, host   \n",
       "\n",
       "                                           utterance             program  \\\n",
       "0  It's a 2,200-mile race. To give some sense of ...  talk of the nation   \n",
       "1  So for a top competitor like Lance to try to m...  talk of the nation   \n",
       "2  So in every team, presumably there's one star,...  talk of the nation   \n",
       "3  That's right. Each team has nine riders. And w...  talk of the nation   \n",
       "4  So slipstream, this is like drafting in car ra...  talk of the nation   \n",
       "\n",
       "                             title episode_date  is_host  \n",
       "0  how to watch the tour de france   2010-07-12    False  \n",
       "1  how to watch the tour de france   2010-07-12    False  \n",
       "2  how to watch the tour de france   2010-07-12     True  \n",
       "3  how to watch the tour de france   2010-07-12    False  \n",
       "4  how to watch the tour de france   2010-07-12     True  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = acquire.get_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nulls\n",
    "- duplicates\n",
    "- create sentiment score\n",
    "- use prep functions\n",
    "- keep some punctuation\n",
    "- split\n",
    "- date to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # obtain top 10 hosts\n",
    "# hosts_to_keep = df[df.is_host == True].speaker.value_counts().head(10).index.to_list()\n",
    "# # create host df\n",
    "# hosts_df = df[df.speaker.isin(hosts_to_keep)]\n",
    "# # get episode_id of top 10 hosts\n",
    "# top_host_episodes = hosts_df.episode_id.value_counts().index.to_list()\n",
    "# # create dataframe with mask of episodes with top hosts\n",
    "# df = df[df.episode_id.isin(top_host_episodes)]\n",
    "# # remove rows with foreign languages spoken\n",
    "# df = df[df.utterance!='(Foreign language spoken)']\n",
    "# # remove rows without speaker (sound effects)\n",
    "# df = df[df.speaker!='_NO_SPEAKER']\n",
    "# # drop duplicates\n",
    "# df.drop_duplicates(inplace = True)\n",
    "# # drop nulls\n",
    "# df.dropna(inplace=True)\n",
    "# # create clean column\n",
    "# df['clean'] = [tokenize(basic_clean(u)) for u in df.utterance]\n",
    "# # create lemmatized column\n",
    "# df['lemmatized'] = df['clean'].apply(tokenize).apply(lemmatize)\n",
    "# # vader sentiment analysis\n",
    "# sia = nltk.sentiment.SentimentIntensityAnalyzer()\n",
    "# df['vader'] = df.lemmatized.apply(lambda doc: sia.polarity_scores(doc)['compound'])\n",
    "# # date column to datetime\n",
    "# df['date'] = pd.to_datetime(df.episode_date)\n",
    "# # cutoff dates prior to 2005 due to low observation count\n",
    "# df = df[df.date > '2005']\n",
    "# # double check drop nulls\n",
    "# df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # obtain top 10 hosts\n",
    "# hosts_to_keep = df[df.is_host == True].speaker.value_counts().head(10).index.to_list()\n",
    "# # create host df\n",
    "# hosts_df = df[df.speaker.isin(hosts_to_keep)]\n",
    "# # get episode_id of top 10 hosts\n",
    "# top_host_episodes = hosts_df.episode_id.value_counts().index.to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe with mask of episodes with top hosts\n",
    "# df = df[df.episode_id.isin(top_host_episodes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop nulls\n",
    "# df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates\n",
    "# df.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove rows without speaker (sound effects)\n",
    "# df = df[df.speaker!='_NO_SPEAKER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove rows with foreign languages spoken\n",
    "# df = df[df.utterance!='(Foreign language spoken)']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create clean column\n",
    "# df['clean'] = [tokenize(basic_clean(u)) for u in df.utterance]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create lemmatized column\n",
    "# df['lemmatized'] = df['clean'].apply(tokenize).apply(lemmatize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>episode_order</th>\n",
       "      <th>speaker</th>\n",
       "      <th>utterance</th>\n",
       "      <th>program</th>\n",
       "      <th>title</th>\n",
       "      <th>is_host</th>\n",
       "      <th>clean</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>vader</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57264</td>\n",
       "      <td>9</td>\n",
       "      <td>ms. loren mooney (editor-in-chief, bicycling m...</td>\n",
       "      <td>It's a 2,200-mile race. To give some sense of ...</td>\n",
       "      <td>talk of the nation</td>\n",
       "      <td>how to watch the tour de france</td>\n",
       "      <td>False</td>\n",
       "      <td>it s a 2,200 mile race. to give some sense of ...</td>\n",
       "      <td>it s a 2,200 mile race. to give some sense of ...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2010-07-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57264</td>\n",
       "      <td>10</td>\n",
       "      <td>ms. loren mooney (editor-in-chief, bicycling m...</td>\n",
       "      <td>So for a top competitor like Lance to try to m...</td>\n",
       "      <td>talk of the nation</td>\n",
       "      <td>how to watch the tour de france</td>\n",
       "      <td>False</td>\n",
       "      <td>so for a top competitor like lance to try to m...</td>\n",
       "      <td>so for a top competitor like lance to try to m...</td>\n",
       "      <td>0.9346</td>\n",
       "      <td>2010-07-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57264</td>\n",
       "      <td>11</td>\n",
       "      <td>neal conan, host</td>\n",
       "      <td>So in every team, presumably there's one star,...</td>\n",
       "      <td>talk of the nation</td>\n",
       "      <td>how to watch the tour de france</td>\n",
       "      <td>True</td>\n",
       "      <td>so in every team , presumably there s one star...</td>\n",
       "      <td>so in every team , presumably there s one star...</td>\n",
       "      <td>0.7096</td>\n",
       "      <td>2010-07-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>57264</td>\n",
       "      <td>12</td>\n",
       "      <td>ms. loren mooney (editor-in-chief, bicycling m...</td>\n",
       "      <td>That's right. Each team has nine riders. And w...</td>\n",
       "      <td>talk of the nation</td>\n",
       "      <td>how to watch the tour de france</td>\n",
       "      <td>False</td>\n",
       "      <td>that s right. each team has nine riders. and w...</td>\n",
       "      <td>that s right. each team ha nine riders. and wh...</td>\n",
       "      <td>0.9274</td>\n",
       "      <td>2010-07-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57264</td>\n",
       "      <td>13</td>\n",
       "      <td>neal conan, host</td>\n",
       "      <td>So slipstream, this is like drafting in car ra...</td>\n",
       "      <td>talk of the nation</td>\n",
       "      <td>how to watch the tour de france</td>\n",
       "      <td>True</td>\n",
       "      <td>so slipstream , this is like drafting in car r...</td>\n",
       "      <td>so slipstream , this is like drafting in car r...</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>2010-07-12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode_id  episode_order  \\\n",
       "0       57264              9   \n",
       "1       57264             10   \n",
       "2       57264             11   \n",
       "3       57264             12   \n",
       "4       57264             13   \n",
       "\n",
       "                                             speaker  \\\n",
       "0  ms. loren mooney (editor-in-chief, bicycling m...   \n",
       "1  ms. loren mooney (editor-in-chief, bicycling m...   \n",
       "2                                   neal conan, host   \n",
       "3  ms. loren mooney (editor-in-chief, bicycling m...   \n",
       "4                                   neal conan, host   \n",
       "\n",
       "                                           utterance             program  \\\n",
       "0  It's a 2,200-mile race. To give some sense of ...  talk of the nation   \n",
       "1  So for a top competitor like Lance to try to m...  talk of the nation   \n",
       "2  So in every team, presumably there's one star,...  talk of the nation   \n",
       "3  That's right. Each team has nine riders. And w...  talk of the nation   \n",
       "4  So slipstream, this is like drafting in car ra...  talk of the nation   \n",
       "\n",
       "                             title  is_host  \\\n",
       "0  how to watch the tour de france    False   \n",
       "1  how to watch the tour de france    False   \n",
       "2  how to watch the tour de france     True   \n",
       "3  how to watch the tour de france    False   \n",
       "4  how to watch the tour de france     True   \n",
       "\n",
       "                                               clean  \\\n",
       "0  it s a 2,200 mile race. to give some sense of ...   \n",
       "1  so for a top competitor like lance to try to m...   \n",
       "2  so in every team , presumably there s one star...   \n",
       "3  that s right. each team has nine riders. and w...   \n",
       "4  so slipstream , this is like drafting in car r...   \n",
       "\n",
       "                                          lemmatized   vader        date  \n",
       "0  it s a 2,200 mile race. to give some sense of ...  0.0000  2010-07-12  \n",
       "1  so for a top competitor like lance to try to m...  0.9346  2010-07-12  \n",
       "2  so in every team , presumably there s one star...  0.7096  2010-07-12  \n",
       "3  that s right. each team ha nine riders. and wh...  0.9274  2010-07-12  \n",
       "4  so slipstream , this is like drafting in car r...  0.3612  2010-07-12  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting top hosts...\n",
      "Getting Episode ID's for the hosts...\n",
      "Double checking speaker variables...\n",
      "Dropping duplicates...\n",
      "Dropping null values...\n",
      "Cleaning corpus...\n",
      "Lemmatizing corpus...\n",
      "Analyzing sentiment with VADER...\n",
      "Converting to datetime...\n",
      "Trimming timeline...\n",
      "The df has 1889857 rows and 12 columns.\n"
     ]
    }
   ],
   "source": [
    "# df = wrangle.get_npr_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JOSH\n",
    "- Are there words that are said more frequently by hosts? By time of day? By category?\n",
    "What host(s) say(s) the most words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The df has 1889857 rows and 12 columns.\n"
     ]
    }
   ],
   "source": [
    "df = wrangle.get_npr_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemmatized'] = df.lemmatized.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = wrangle.split_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neal conan, host',\n",
       " 'ira flatow, host',\n",
       " 'steve inskeep, host',\n",
       " 'robert siegel, host',\n",
       " 'melissa block, host',\n",
       " 'renee montagne, host',\n",
       " 'farai chideya, host',\n",
       " 'scott simon, host',\n",
       " 'rachel martin, host',\n",
       " 'david greene, host']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df of hosts\n",
    "host_df = df[df.is_host==True]\n",
    "# hosts with most interviews\n",
    "hosts_with_the_most = host_df.speaker.value_counts().head(10).index.to_list()\n",
    "hosts_with_the_most\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate utterances by the speakers that are in top 10 hosts\n",
    "host_words = train[train.speaker.isin(hosts_with_the_most)].groupby('speaker')['clean'].agg(lambda col: ' '.join(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_cleaning(s):\n",
    "    '''\n",
    "    Function to remove punctuation from word frequencies for hosts.\n",
    "    '''\n",
    "    # remove special characters\n",
    "    s = re.sub(r\"[^a-z0-9'\\s]\", '', s)\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_words = host_words.apply(string_cleaning).apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create word frequencies for each host\n",
    "greene_freq = pd.Series(host_words['david greene, host'].split()).value_counts()\n",
    "martin_freq = pd.Series(host_words['rachel martin, host'].split()).value_counts()\n",
    "simon_freq = pd.Series(host_words['scott simon, host'].split()).value_counts()\n",
    "chideya_freq = pd.Series(host_words['farai chideya, host'].split()).value_counts()\n",
    "montagne_freq = pd.Series(host_words['renee montagne, host'].split()).value_counts()\n",
    "block_freq = pd.Series(host_words['melissa block, host'].split()).value_counts()\n",
    "siegel_freq = pd.Series(host_words['robert siegel, host'].split()).value_counts()\n",
    "inskeep_freq = pd.Series(host_words['steve inskeep, host'].split()).value_counts()\n",
    "flatow_freq = pd.Series(host_words['ira flatow, host'].split()).value_counts()\n",
    "conan_freq = pd.Series(host_words['neal conan, host'].split()).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conan</th>\n",
       "      <th>flatow</th>\n",
       "      <th>inskeep</th>\n",
       "      <th>siegel</th>\n",
       "      <th>block</th>\n",
       "      <th>montagne</th>\n",
       "      <th>chideya</th>\n",
       "      <th>simon</th>\n",
       "      <th>martin</th>\n",
       "      <th>greene</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>us</th>\n",
       "      <td>39584</td>\n",
       "      <td>5614</td>\n",
       "      <td>2824</td>\n",
       "      <td>4716</td>\n",
       "      <td>2673</td>\n",
       "      <td>3294</td>\n",
       "      <td>3200</td>\n",
       "      <td>5038</td>\n",
       "      <td>2991</td>\n",
       "      <td>2054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talk</th>\n",
       "      <td>18267</td>\n",
       "      <td>3861</td>\n",
       "      <td>923</td>\n",
       "      <td>552</td>\n",
       "      <td>800</td>\n",
       "      <td>448</td>\n",
       "      <td>996</td>\n",
       "      <td>419</td>\n",
       "      <td>522</td>\n",
       "      <td>564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>much</th>\n",
       "      <td>14374</td>\n",
       "      <td>1225</td>\n",
       "      <td>2217</td>\n",
       "      <td>1425</td>\n",
       "      <td>1340</td>\n",
       "      <td>1435</td>\n",
       "      <td>1282</td>\n",
       "      <td>2542</td>\n",
       "      <td>1498</td>\n",
       "      <td>801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>well</th>\n",
       "      <td>14351</td>\n",
       "      <td>2572</td>\n",
       "      <td>1559</td>\n",
       "      <td>2417</td>\n",
       "      <td>1505</td>\n",
       "      <td>1096</td>\n",
       "      <td>2121</td>\n",
       "      <td>905</td>\n",
       "      <td>540</td>\n",
       "      <td>1290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>let</th>\n",
       "      <td>13896</td>\n",
       "      <td>3366</td>\n",
       "      <td>1610</td>\n",
       "      <td>622</td>\n",
       "      <td>617</td>\n",
       "      <td>541</td>\n",
       "      <td>1541</td>\n",
       "      <td>682</td>\n",
       "      <td>749</td>\n",
       "      <td>972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>charbroiling</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ravech</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>majook</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stewardships</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ooos</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82915 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              conan  flatow  inskeep  siegel  block  montagne  chideya  simon  \\\n",
       "us            39584    5614     2824    4716   2673      3294     3200   5038   \n",
       "talk          18267    3861      923     552    800       448      996    419   \n",
       "much          14374    1225     2217    1425   1340      1435     1282   2542   \n",
       "well          14351    2572     1559    2417   1505      1096     2121    905   \n",
       "let           13896    3366     1610     622    617       541     1541    682   \n",
       "...             ...     ...      ...     ...    ...       ...      ...    ...   \n",
       "charbroiling      0       0        0       0      0         0        0      0   \n",
       "ravech            0       0        0       0      0         0        0      0   \n",
       "majook            0       0        0       0      0         0        0      0   \n",
       "stewardships      0       0        0       0      0         0        0      0   \n",
       "ooos              0       0        0       0      0         0        0      0   \n",
       "\n",
       "              martin  greene  \n",
       "us              2991    2054  \n",
       "talk             522     564  \n",
       "much            1498     801  \n",
       "well             540    1290  \n",
       "let              749     972  \n",
       "...              ...     ...  \n",
       "charbroiling       0       1  \n",
       "ravech             0       1  \n",
       "majook             0       1  \n",
       "stewardships       0       1  \n",
       "ooos               0       1  \n",
       "\n",
       "[82915 rows x 10 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = pd.concat([conan_freq, flatow_freq, inskeep_freq, siegel_freq, block_freq, montagne_freq, chideya_freq, simon_freq, martin_freq, greene_freq], axis=1).fillna(0).astype(int)\n",
    "word_counts.columns = ['conan', 'flatow', 'inskeep', 'siegel', 'block', 'montagne', 'chideya', 'simon', 'martin', 'greene']\n",
    "word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary for word clouds\n",
    "word_lists = {'conan':conan_freq, \n",
    "'flatow':flatow_freq, \n",
    "'inskeep': inskeep_freq, \n",
    "'siegel':siegel_freq, \n",
    "'block':block_freq, \n",
    "'montagne':montagne_freq, \n",
    "'chideya':chideya_freq, \n",
    "'simon':simon_freq, \n",
    "'martin':martin_freq, \n",
    "'greene':greene_freq}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-3379cfdefa2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_lists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackground_color\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'white'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_lists\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/lib/python3.8/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \"\"\"\n\u001b[0;32m--> 632\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/lib/python3.8/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         \"\"\"\n\u001b[0;32m--> 613\u001b[0;31m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/lib/python3.8/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mprocess_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0mregexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregexp\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregexp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m         \u001b[0;31m# remove 's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         words = [word[:-2] if word.lower().endswith(\"'s\") else word\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/lib/python3.8/re.py\u001b[0m in \u001b[0;36mfindall\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     Empty matches are included in the result.\"\"\"\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 504x504 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# word clouds\n",
    "for h in word_lists:\n",
    "    plt.figure(figsize=(7,7))\n",
    "    img = WordCloud(background_color='white', width=800, height=600).generate(word_lists[h])\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(f'{h}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'float' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-ab083654e126>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word_count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasic_clean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mvalidate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message_length'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word_count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasic_clean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4136\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4137\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4138\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'float' has no len()"
     ]
    }
   ],
   "source": [
    "train['message_length'] = train.clean.apply(len)\n",
    "train['word_count'] = train.clean.apply(basic_clean).apply(str.split).apply(len)\n",
    "\n",
    "validate['message_length'] = validate.clean.apply(len)\n",
    "validate['word_count'] = validate.clean.apply(basic_clean).apply(str.split).apply(len)\n",
    "\n",
    "test['message_length'] = test.clean.apply(len)\n",
    "test['word_count'] = test.clean.apply(basic_clean).apply(str.split).apply(len)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speaker\n",
      "david greene, host      178.760259\n",
      "farai chideya, host     190.173800\n",
      "ira flatow, host        122.776262\n",
      "melissa block, host     164.656432\n",
      "neal conan, host        148.899234\n",
      "rachel martin, host     157.298332\n",
      "renee montagne, host    171.041212\n",
      "robert siegel, host     174.635140\n",
      "scott simon, host       181.393062\n",
      "steve inskeep, host     168.979687\n",
      "Name: message_length, dtype: float64\n",
      "speaker\n",
      "david greene, host      34.854750\n",
      "farai chideya, host     37.695985\n",
      "ira flatow, host        25.493543\n",
      "melissa block, host     31.889422\n",
      "neal conan, host        30.021699\n",
      "rachel martin, host     30.763272\n",
      "renee montagne, host    32.494347\n",
      "robert siegel, host     33.524589\n",
      "scott simon, host       35.161734\n",
      "steve inskeep, host     32.412296\n",
      "Name: word_count, dtype: float64\n",
      "speaker\n",
      "david greene, host       555515\n",
      "farai chideya, host      768018\n",
      "ira flatow, host        1044292\n",
      "melissa block, host      711453\n",
      "neal conan, host        4293223\n",
      "rachel martin, host      523837\n",
      "renee montagne, host     672568\n",
      "robert siegel, host      931883\n",
      "scott simon, host        662869\n",
      "steve inskeep, host     1072296\n",
      "Name: word_count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train[train.speaker.isin(hosts_with_the_most)].groupby('speaker').message_length.mean())\n",
    "print(train[train.speaker.isin(hosts_with_the_most)].groupby('speaker').word_count.mean())\n",
    "print(train[train.speaker.isin(hosts_with_the_most)].groupby('speaker').word_count.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speaker              episode_id\n",
       "david greene, host   777           1742\n",
       "                     780            189\n",
       "                     800            667\n",
       "                     802            708\n",
       "                     804            489\n",
       "                                   ... \n",
       "steve inskeep, host  133180          18\n",
       "                     133182          23\n",
       "                     133184         453\n",
       "                     135547         365\n",
       "                     138265        1624\n",
       "Name: message_length, Length: 56725, dtype: int64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train.speaker.isin(hosts_with_the_most)].groupby(['speaker','episode_id']).message_length.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speaker              episode_id\n",
       "david greene, host   777           335\n",
       "                     780            38\n",
       "                     800           121\n",
       "                     802           141\n",
       "                     804            91\n",
       "                                  ... \n",
       "steve inskeep, host  133180          5\n",
       "                     133182          6\n",
       "                     133184         80\n",
       "                     135547         69\n",
       "                     138265        311\n",
       "Name: word_count, Length: 56725, dtype: int64"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train.speaker.isin(hosts_with_the_most)].groupby(['speaker','episode_id']).word_count.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
